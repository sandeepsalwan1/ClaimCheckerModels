{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "column_names = [\n",
    "    'id', 'label', 'statement', 'subject', 'speaker', 'speaker_job',\n",
    "    'state', 'party', 'barely_true_counts', 'false_counts',\n",
    "    'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "train_df = pd.read_csv('liar_dataset (2)/train.tsv', sep='\\t', names=column_names)\n",
    "valid_df = pd.read_csv('liar_dataset (2)/valid.tsv', sep='\\t', names=column_names)\n",
    "test_df = pd.read_csv('liar_dataset (2)/test.tsv', sep='\\t', names=column_names)\n",
    "\n",
    "# Convert labels to binary\n",
    "def binarize_label(label):\n",
    "    return 1 if label in ['true', 'mostly-true'] else 0\n",
    "\n",
    "train_df['binary_label'] = train_df['label'].apply(binarize_label)\n",
    "valid_df['binary_label'] = valid_df['label'].apply(binarize_label)\n",
    "test_df['binary_label'] = test_df['label'].apply(binarize_label)\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_df['statement']\n",
    "y_train = train_df['binary_label']\n",
    "X_valid = valid_df['statement']\n",
    "y_valid = valid_df['binary_label']\n",
    "X_test = test_df['statement']\n",
    "y_test = test_df['binary_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_job</th>\n",
       "      <th>state</th>\n",
       "      <th>party</th>\n",
       "      <th>barely_true_counts</th>\n",
       "      <th>false_counts</th>\n",
       "      <th>half_true_counts</th>\n",
       "      <th>mostly_true_counts</th>\n",
       "      <th>pants_on_fire_counts</th>\n",
       "      <th>context</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        label                                          statement  \\\n",
       "0   2635.json        false  Says the Annies List political group supports ...   \n",
       "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
       "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                              subject         speaker           speaker_job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "      state       party  barely_true_counts  false_counts  half_true_counts  \\\n",
       "0     Texas  republican                 0.0           1.0               0.0   \n",
       "1  Virginia    democrat                 0.0           0.0               1.0   \n",
       "2  Illinois    democrat                70.0          71.0             160.0   \n",
       "3       NaN        none                 7.0          19.0               3.0   \n",
       "4   Florida    democrat                15.0           9.0              20.0   \n",
       "\n",
       "   mostly_true_counts  pants_on_fire_counts              context  binary_label  \n",
       "0                 0.0                   0.0             a mailer             0  \n",
       "1                 1.0                   0.0      a floor speech.             0  \n",
       "2               163.0                   9.0               Denver             1  \n",
       "3                 5.0                  44.0       a news release             0  \n",
       "4                19.0                   2.0  an interview on CNN             0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6192\n",
      "Validation Precision: 0.4340\n",
      "Validation Recall: 0.5405\n",
      "Validation F1 Score: 0.4814\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.66      0.70       864\n",
      "           1       0.43      0.54      0.48       420\n",
      "\n",
      "    accuracy                           0.62      1284\n",
      "   macro avg       0.59      0.60      0.59      1284\n",
      "weighted avg       0.64      0.62      0.63      1284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF vectorizer - converts text to numbers\n",
    "# Example: For \"Tax cuts created jobs\" → creates a vocabulary and assigns weights to words\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit and transform training data - learns vocabulary and converts training claims to vectors\n",
    "# Example: \"Tax cuts created jobs\" → [0.4, 0.3, 0.5, 0.6, 0, 0, ...] (in 5000-dimensional space)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform validation and test data - uses existing vocabulary to convert new claims\n",
    "# Example: For new claim \"Jobs grew last year\" → [0, 0, 0, 0.7, 0, 0, ...] (same 5000 dimensions)\n",
    "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Create and train logistic regression model - learns weights for each word to predict truthfulness\n",
    "# Example: Learns that words like \"unemployment\" with \"lowest\" may indicate truth\n",
    "baseline_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "baseline_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on validation set - applies learned weights to new claims\n",
    "# Example: For vector [0, 0, 0, 0.7, 0, 0, ...] → predicts \"True\" (1) or \"False\" (0)\n",
    "y_valid_pred = baseline_model.predict(X_valid_tfidf)\n",
    "y_valid_prob = baseline_model.predict_proba(X_valid_tfidf)[:, 1]  # Probability of being true (class 1)\n",
    "\n",
    "# Calculate validation metrics - measures how well the model performed\n",
    "# Example: If prediction was 1 but actual was 0 → contributes to lower accuracy\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred)  # Percentage of correct predictions\n",
    "valid_precision, valid_recall, valid_f1, _ = precision_recall_fscore_support(\n",
    "   y_valid, y_valid_pred, average='binary'  # Gets metrics for the \"True\" (1) class\n",
    ")\n",
    "\n",
    "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {valid_precision:.4f}\")\n",
    "print(f\"Validation Recall: {valid_recall:.4f}\")\n",
    "print(f\"Validation F1 Score: {valid_f1:.4f}\")\n",
    "print(\"\\nValidation Classification Report:\")\n",
    "print(classification_report(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Analysis (Logistic Regression with TF-IDF)\n",
    "\n",
    "The baseline model achieves the following performance metrics:\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| False (0) | 75% | 66% | 70% | 864 |\n",
    "| True (1) | 43% | 54% | 48% | 420 |\n",
    "| **Overall** | 64% | 62% | 63% | 1284 |\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Accuracy (62%)**: The model correctly classifies 62% of all claims.\n",
    "\n",
    "- **For False Claims**:\n",
    "  - **Precision (75%)**: When the model predicts a claim is false, it's correct 75% of the time.\n",
    "  - **Recall (66%)**: The model successfully identifies 66% of all false claims.\n",
    "  - **F1-Score (70%)**: The harmonic mean of precision and recall, providing a balanced measure of the model's performance on false claims.\n",
    "\n",
    "- **For True Claims**:\n",
    "  - **Precision (43%)**: When the model predicts a claim is true, it's correct only 43% of the time.\n",
    "  - **Recall (54%)**: The model identifies 54% of all true claims.\n",
    "  - **F1-Score (48%)**: The harmonic mean of precision and recall, showing moderate performance on true claims.\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. The model performs better on false claims than true claims, likely due to class imbalance (more false claims in the dataset).\n",
    "\n",
    "2. The lower precision for true claims indicates a higher rate of false positives - the model tends to incorrectly classify false claims as true.\n",
    "\n",
    "3. The F1-score provides a single metric that balances precision and recall. The overall F1-score of 63% will serve as our primary comparison metric for more sophisticated models.\n",
    "\n",
    "This baseline establishes a minimum performance threshold that our deep learning models should exceed to demonstrate effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5825\n",
      "Test Precision: 0.4315\n",
      "Test Recall: 0.5612\n",
      "Test F1 Score: 0.4879\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.59      0.65       818\n",
      "           1       0.43      0.56      0.49       449\n",
      "\n",
      "    accuracy                           0.58      1267\n",
      "   macro avg       0.57      0.58      0.57      1267\n",
      "weighted avg       0.61      0.58      0.59      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "y_test_pred = baseline_model.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_test_pred, average='binary'\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Test Results (Logistic Regression with TF-IDF)\n",
    "\n",
    "Our baseline logistic regression model with TF-IDF features achieved the following performance metrics on the test set:\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|-----------|--------|----------|---------|\n",
    "| False (0) | 71% | 59% | 65% | 818 |\n",
    "| True (1) | 43% | 56% | 49% | 449 |\n",
    "| **Overall** | 61% | 58% | 59% | 1267 |\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Accuracy: 58.25%** - The model correctly classifies just over half of all claims.\n",
    "\n",
    "- **False Claims Performance:**\n",
    "  - When the model predicts a claim is false, it's right 71% of the time.\n",
    "  - The model identifies 59% of all false claims in the dataset.\n",
    "  - F1-score of 65% indicates reasonable but not excellent performance.\n",
    "\n",
    "- **True Claims Performance:**\n",
    "  - When the model predicts a claim is true, it's right only 43% of the time.\n",
    "  - The model identifies 56% of all true claims in the dataset.\n",
    "  - F1-score of 49% shows the model struggles more with identifying true claims accurately.\n",
    "\n",
    "- **Class Imbalance Effect:**\n",
    "  - The test set has 818 false claims and 449 true claims (a 1.8:1 ratio).\n",
    "  - This imbalance likely contributes to the model's weaker performance on true claims.\n",
    "\n",
    "### Benchmark Targets for Advanced Models:\n",
    "\n",
    "Our deep learning approaches should aim to exceed:\n",
    "- **Overall Accuracy: > 58.25%**\n",
    "- **F1-Score for True Claims: > 49%**\n",
    "- **F1-Score for False Claims: > 65%**\n",
    "- **Overall F1-Score: > 59%**\n",
    "- **Precision: > 61%**\n",
    "\n",
    "These metrics establish the minimum performance thresholds that more sophisticated models must surpass to demonstrate meaningful improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
